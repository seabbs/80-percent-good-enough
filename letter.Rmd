---
title: What is 80% good enough for real-time analyses during the COVID-19 pandemic?
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
output: pdf_document
bibliography: [library.bib]
---

## Abstract

The COVID-19 pandemic has seen an unprecedented use of infectious disease research conducted over short time scales as a tool for setting health policy. However, it has been widely acknowledged that some of this work is flawed, both in ways that are reported and due to time constraints and in unreported ways. Here we explore some of the underlying reasons for this using examples of our work. We find that keys issues include: lack of technical and statistical knowledge, poor availability of domain-specific tools, difficulty in supporting long term incremental projects, and lack of support for incremental advance. We conclude that researchers should be more open about the reasons for limitations in their work and that more focus and support should be given for evaluation, incremental improvement and the development of domain-specific tools.

## Main text

"An 80% right paper before a policy decision is made is worth ten 95% right papers afterwards, provided the methodological limitations imposed by doing it fast are made clear."

This quote from Whitty et al. [@whitty_what_2015] captures some of the academic discourse [@brooks-pollock] around real-time analyses conducted to inform COVID-19 health policy. Here we use examples from our policy adjacent COVID-19 work to explore the issues leading to an analysis being "80% correct" and discuss the factors that led to these issues.

Collectively, we have published numerous studies related to the COVID-19 pandemic, have contributed studies to government advisory committees, presented results to the WHO and other bodies, and have worked on previous infectious disease outbreaks with similar issues such as the 2013-2016 Ebola outbreak in West Africa. Our work spans non-parametric statistical modelling of transmission, short-term forecasts, scenario modelling using mechanistic compartmental models, and a range of reactive analyses usually conducted with short-time horizons to inform policy-makers.

The first example of our work we discuss is a project to estimate the effective reproduction number of COVID-19 in real-time [@Abbott2020v1; @Abbott2020v2]. Starting in February 2020 with a focus on China and growing to include national estimates globally as well as estimates at smaller scales in multiple countries with a range of data sources. This work has fed into the Scientific Pandemic Influenza Group on Modelling (SPI-M) aggregated reproduction number estimate and short-term forecasts [@sherratt_exploring_2021; @funk_short-term_2020] and the site where estimates are presented has had over 400,000 unique users. As based on surveillance data, done at a large scale, and repeated on a routine basis this work has many obvious limitations. In our published work, we reported a range of these including the lack of location-specific data on the time from infection to case report, lack of clarity on the interaction between the generation time and the reproduction number, and the difficulty in extrapolating current reported cases to infer the dynamics of current infections. However, our initial work also included an unknown, and hence unreported, limitation in that it assumed that the delay from infection to report could be treated as reversible[@Abbott2020v1]. This has since been investigated extensively and has been shown to result in over-smoothed estimates that lag real-world changes in transmission [@gostic]. Though this issue has since been mitigated via methodological developments [@Abbott2020v2] it is likely that it resulted in flawed inferences.

A critical issue that influenced the initial flawed implementation was a lack of knowledge of developments in certain areas of the literature where this issue was known[@gostic]. This was potentially the result of a lack of experience within our team (as much of the development work was done by early career researchers) but also the result of the failure of those working in the areas where this had previously been explored to propagate robust tooling and methodology. A wider issue with this project was its continuous nature and lack of discrete outcomes which made incentivising contributions difficult, especially once the initial work had been published. In theory, this could have been resolved by building collaborations with those developing new methodology as contributors to the project but in practice, this was difficult to negotiate despite significant time and resources being spent on this aim. Unfortunately whilst a range of real-time reproduction number estimation methods now exist in general these have been only partially evaluated and they are rarely designed with reusability, robustness, or routine usage in mind.

A related example is two linked studies that were conducted in December 2020 and June 2021 to estimate the transmissibility advantage of the Alpha and Delta variants of COVID-19 [@Davies2021; @AbbottDelta]. Both of these analyses used the same underlying regression framework to model reproduction numbers as a mixture of the variant of concern and the other variants reproduction numbers (with this only being a small part of the analyses in Davies et al.[@Davies2021]). This allowed for a multiplicative transmission advantage to be estimated after adjusting for static and time-varying variables influencing transmission. These analyses were presented to SPI-M and the Scientific Advisory Group for Emergencies (SAGE) with the first being widely reported in the media, often as a point estimate. These analyses reported a range of limitations including the use of the expected reproduction number and proportion of cases that had the variant of concern, the use of S-gene target failure as a proxy for variant status, the lack of UK specific generation time estimates, the lack of modelling spatial auto-correlation explicitly, the usage of non-parametric reproduction number estimates rather than a single model, and the assumption that the generation time was unchanged between variants. These issues were likely to have led to spuriously precise results though the magnitude of this is difficult to quantify. Bias may have been introduced by using the mean estimated reproduction numbers and mean proportion of cases with the variant of concern as this weights all areas equally. Bias may also have been introduced due to the assumption that the generation time was unchanged between variants.

The issues with these studies are perhaps less critical than in the first example. However, they do highlight a general theme in COVID-19 analysis, quantification of uncertainty[@zelner] or lack thereof. It is difficult to assess the impact of these omissions, without replication of these studies with improved methods. In this instance, this is a work in progress[@Abbott:pr] but in general, this is rarely done. This is likely because it is difficult to justify as novel work and because it can require substantially more complex methods. The limitations in this example were due to lack of capacity, both in terms of researcher time and technical infrastructure such as compute resources, and lack of statistical expertise. It is important to note that the analysis was repeated effectively unchanged when concerns were raised about the Delta variant despite several months having passed in which improvements could have been made. This was again largely due to lack of capacity as other questions of interest had been raised in the interim but given the likely development of new variants, this was an oversight. The limitations of this example were mitigated in the first instance by including the analysis in a collection of other estimates derived using various techniques [@Davies2021] and in the second instance by comparison to a range of independent studies when aggregated at SPI-M. Unfortunately however many of these approaches made similar simplifications to streamline analysis and hence were likely biased and spuriously precise in similar areas. Within these studies, some of the limitations were partially addressed by reporting results for a range of models and a range of generation times though this was difficult to communicate to those making use of the estimates.

*Example 3*

*Example 4*

The examples we have highlighted all share several common themes with the most common being limited breadth of knowledge in research groups. This lack of knowledge extends from domain knowledge to statistical and technical skills, such as software engineering, required to handle data. These skills are especially important for real-time analyses in which results are required quickly and data sources can be complex. This issue is compounded by the lack of well-developed software tools designed to be used flexibly for real-world analysis questions. Many of the currently available generic open-source tools, whilst very high quality, require a substantial level of user knowledge and for relatively common domain-specific problems can require significant researcher time to produce results. Most of the available domain-specific tools have significant limitations or are not continuously supported. Another common theme is the difficulty in collaborating across groups on rapidly developing problems and on incremental method development more generally.

Whilst it is difficult to define what is good enough for real-time analyses here we have summarised some of the common causes of limitations with our work. 
It is plausible that the issues highlighted are not generalisable to other researchers and indeed a potential conclusion is that we should not be conducting real-time analysis. However, we feel that the issues highlighted are more systematic than this and point to persistent structural flaws within the academic system[@kucharski_covid-19_2020]. Many of the issues we have highlighted are exacerbated by an academic culture that promotes individual scientific achievement over collaborative incremental development, does not promote a culture of ongoing technical and statistical training, and indeed does not reward or value these skills in applied researchers. All of the examples in this letter were published with open-source code. In theory, this should allow other researchers to explore these analyses and mitigate some of the limitations but in practice, incentive structures favour new work rather than incremental improvements and evaluation. We feel it is vital that we and others evaluate real-time analyses, improve on the methodology used, and expand the supply of domain-specific tooling so that future work can be of higher quality. This can most easily be done when researchers are open and honest about the limitations of their work and the causes of these limitations and funders are supportive of this goal.

## References

<div id = 'refs'></div>
